{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
      "  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 592.3MB 73.0MB/s ta 0:00:011 2% |█                               | 17.7MB 46.2MB/s eta 0:00:13    8% |██▊                             | 50.8MB 13.7MB/s eta 0:00:40    35% |███████████▍                    | 210.2MB 8.3MB/s eta 0:00:47    42% |█████████████▋                  | 252.5MB 31.5MB/s eta 0:00:11    57% |██████████████████▌             | 343.2MB 58.6MB/s eta 0:00:05    62% |████████████████████            | 371.5MB 11.1MB/s eta 0:00:20    64% |████████████████████▌           | 379.7MB 16.1MB/s eta 0:00:14MB 90.6MB/s eta 0:00:02    86% |███████████████████████████▊    | 512.9MB 22.8MB/s eta 0:00:048% |████████████████████████████▎   | 523.1MB 74.0MB/s eta 0:00:01███████▌ | 563.5MB 95.5MB/s eta 0:00:01████████ | 571.8MB 80.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Using cached torchvision-0.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages\n",
      "Collecting pyyaml (from torch==0.3.0.post4)\n",
      "Collecting pillow>=4.1.1 (from torchvision)\n",
      "  Using cached Pillow-5.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/conda/lib/python3.6/site-packages (from pandas)\n",
      "Installing collected packages: pyyaml, torch, pillow, torchvision\n",
      "Successfully installed pillow-5.0.0 pyyaml-3.12 torch-0.3.0.post4 torchvision-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl torchvision pandas numpy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting quandl\n",
      "  Using cached Quandl-3.3.0-py2.py3-none-any.whl\n",
      "Collecting ndg-httpsclient (from quandl)\n",
      "  Using cached ndg_httpsclient-0.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.8 in /opt/conda/lib/python3.6/site-packages (from quandl)\n",
      "Collecting pyasn1 (from quandl)\n",
      "  Using cached pyasn1-0.4.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from quandl)\n",
      "Requirement already satisfied: requests>=2.7.0 in /opt/conda/lib/python3.6/site-packages (from quandl)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from quandl)\n",
      "Collecting inflection>=0.3.1 (from quandl)\n",
      "Requirement already satisfied: pandas>=0.14 in /opt/conda/lib/python3.6/site-packages (from quandl)\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.6/site-packages (from quandl)\n",
      "Collecting more-itertools (from quandl)\n",
      "  Using cached more_itertools-4.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.7.0->quandl)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.7.0->quandl)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.7.0->quandl)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.7.0->quandl)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.14->quandl)\n",
      "Requirement already satisfied: cryptography>=1.9 in /opt/conda/lib/python3.6/site-packages (from pyOpenSSL->quandl)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=1.9->pyOpenSSL->quandl)\n",
      "Requirement already satisfied: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=1.9->pyOpenSSL->quandl)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=1.9->pyOpenSSL->quandl)\n",
      "Installing collected packages: pyasn1, ndg-httpsclient, inflection, more-itertools, quandl\n",
      "Successfully installed inflection-0.3.1 more-itertools-4.1.0 ndg-httpsclient-0.4.3 pyasn1-0.4.2 quandl-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Bitcoin Price Prediction                                    \n",
    "#   Here we provide a template to precit bitcoin price and deploy   #\n",
    "#   at scale on a user-defined schedule, taking advantage of        #\n",
    "#   Metis Machine curated data, and external 3rd party data.        #\n",
    "\n",
    "## Import some needed dependencies\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quandl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# ## Set Environment Vars\n",
    "#  `skafos env <variable_name> --set <value>` \n",
    "#    specify what quandl coin data to use and your api key\n",
    "#   We want: CostPerTransaction, NumTransations\n",
    "COIN = \"BCHARTS/BITSTAMPUSD\" \n",
    "API = #<insert API key here>\n",
    "DATASETS = [COIN, \"BCHAIN/CPTRA\", \"BCHAIN/NTRAT\"]\n",
    "\n",
    "## Set the quandl api key so we can access historical coin data\n",
    "quandl.ApiConfig.api_key = API\n",
    "coin_df = quandl.get(DATASETS, returns=\"pandas\")\n",
    "\n",
    "# We are interested in predicting the close price (*6pm of the next day)\n",
    "coin_df = coin_df[['BCHARTS/BITSTAMPUSD - Close', 'BCHAIN/NTRAT - Value', 'BCHAIN/CPTRA - Value']].dropna()\n",
    "coin_df.columns = ['close_price', 'num_trans', 'cost_per_trans']\n",
    "\n",
    "coin_df_end = coin_df.index.max().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the SDK connection\n",
      "2018-01-24 03:48:04,977 - skafossdk.data_engine - INFO - DataEngine Connection Opened\n",
      "2018-01-24 03:48:05,970 - skafossdk.data_engine - INFO - Creating View...\n",
      "2018-01-24 03:48:05,971 - skafossdk.data_engine - INFO - Sending msg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-24 03:48:06,085 - skafossdk.data_engine - ERROR - \"Table(s) [\\\"gtrends\\\"] already exists\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-24 03:48:06,097 - skafossdk.monitor - INFO - Monitor Connection Opened\n",
      "Created a view of historical google trends data\n"
     ]
    }
   ],
   "source": [
    "# Use the Skafos Data Engine to pull in curated dataset\n",
    "from skafossdk import *\n",
    "print('Initializing the SDK connection', flush=True)\n",
    "skafos = Skafos()\n",
    "\n",
    "# Query data engine for google keyword trends\n",
    "res = skafos.engine.create_view(\n",
    "    \"gtrends\", {\"keyspace\": \"google_trends\", \"table\": \"crypto\"}, \n",
    "    DataSourceType.Cassandra).result()\n",
    "print(\"Created a view of historical google trends data\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling in historical google trends data...\n",
      "2018-01-24 03:48:07,115 - skafossdk.data_engine - INFO - Querying ...\n",
      "2018-01-24 03:48:07,117 - skafossdk.data_engine - INFO - Sending msg...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Pulling in historical google trends data...\")\n",
    "gtrends_json = skafos.engine.query(\"SELECT * from gtrends WHERE keyword IN \\\n",
    "('bitcoin', 'blockchain', 'crypto currency', 'litecoin')\").result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating a single record:\n",
      "{'keyword': 'bitcoin', 'date': '2011-12-09T00:00:00Z', 'interest': 0.12286718934774399}\n",
      "2018-01-24 03:48:36,990 - skafossdk.monitor - INFO - Monitor Connection Closed\n"
     ]
    }
   ],
   "source": [
    "# Validate a single record\n",
    "print(\"Validating a single record:\", flush=True)\n",
    "print(gtrends_json['data'][0], flush=True)\n",
    "\n",
    "# Convert to pandas df\n",
    "gtrends = pd.DataFrame.from_records(gtrends_json['data'])\\\n",
    "    .pivot(index='date', values='interest', columns='keyword')\n",
    "\n",
    "# Deal with potential nans\n",
    "for col in gtrends.columns:\n",
    "    if 'NaN' in gtrends[col].values:\n",
    "        gtrends[col].replace({'NaN': None}, inplace=True)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# If there are nans, fill using pad method\n",
    "gtrends.fillna(method='pad', inplace=True)\n",
    "\n",
    "# Set proper date format\n",
    "gtrends.index = pd.to_datetime(gtrends.index)\n",
    "\n",
    "# Catch the last date of gtrends data available\n",
    "gtrends_end = gtrends.index.max().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gtrends Data is 2 days behind. Shifting multiple.\n"
     ]
    }
   ],
   "source": [
    "# Figure out how much we might need to shift based on data availability\n",
    "if (coin_df_end - gtrends_end).days == 0:\n",
    "    #Same day\n",
    "    print(\"Data lined up perfectly, no shifting needed.\")\n",
    "    shifter = 0\n",
    "elif (coin_df_end - gtrends_end).days == 1:\n",
    "    # One day behind\n",
    "    print(\"Gtrends Data is one day behind. Shifting once.\")\n",
    "    shifter = 1\n",
    "else:\n",
    "    # More days behind\n",
    "    shifter = (coin_df_end - gtrends_end).days\n",
    "    print(\"Gtrends Data is %s days behind. Shifting multiple.\" % shifter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join google trends with quandl coin data\n",
    "df = coin_df.join(gtrends, how='left')\n",
    "\n",
    "\n",
    "# ## Prep Inputs for Modeling\n",
    "# We want to use a recurrent time-series model, so our data\n",
    "# need to be in ascending order by date.\n",
    "day_zero = df.index.min()\n",
    "day_index_map = dict(zip((df.index - day_zero).days.values, df.index.values))\n",
    "\n",
    "df.set_index((df.index - day_zero).days, inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Get rid of 0's in price and Calculate percent change in price\n",
    "df = df[df.close_price != df.close_price.min()]\n",
    "df['close_price_change'] = df.close_price.pct_change()\n",
    "\n",
    "# Calculate difference in num trans day to day\n",
    "#df['num_trans'] = df.num_trans.diff(1)\n",
    "\n",
    "# Shift google trends to fill gap in data availability\n",
    "# NOTE: This means the model is using the search volume some x days prior to\n",
    "#       prediction. This should account for human lag in research/interest to action.\n",
    "#       It also may not be as good as one day out or same day ofcourse.\n",
    "df[['bitcoin', 'blockchain', 'litecoin', 'crypto currency']] = df[['bitcoin', 'blockchain', 'litecoin', 'crypto currency']].shift(shifter)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Normalize inputs for deep learning\n",
    "# Most neural networks expect inputs from -1 to 1\n",
    "# So we fit two standard deviations in between  -1 and 1\n",
    "df_scaled = df.apply(lambda c: 0.5 * (c - c.mean()) / c.std())\n",
    "\n",
    "# Shift so that we're trying to predict tomorrow's price\n",
    "bitcoin_y = df_scaled['close_price_change'].copy().shift(-1)\n",
    "\n",
    "bitcoin_x = df_scaled.drop(['close_price'], axis=1)\n",
    "\n",
    "# Predict on the last day\n",
    "last_day = max(bitcoin_x.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Recurrent Neural Network Model\n",
    "# [PyTorch](http://pytorch.org) is a wonderful framnework for deep learning \n",
    "# since it handles backpropgation automatically.\n",
    "\n",
    "x_train = torch.autograd.Variable(\n",
    "    torch.from_numpy(bitcoin_x.loc[:last_day - 1].as_matrix()).float(), requires_grad=False)\n",
    "x_pred = torch.autograd.Variable(\n",
    "    torch.from_numpy(bitcoin_x.loc[last_day:].as_matrix()).float(), requires_grad=False)\n",
    "batch_size = x_train.size()[0]\n",
    "input_size = len(bitcoin_x.columns)\n",
    "\n",
    "\n",
    "y_train = torch.autograd.Variable(\n",
    "    torch.from_numpy(bitcoin_y.loc[:last_day - 1].as_matrix()).float(), requires_grad=False)\n",
    "y_pred = torch.autograd.Variable(\n",
    "    torch.from_numpy(bitcoin_y.loc[last_day:].as_matrix()).float(), requires_grad=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CryptoNet(\n",
      "  (rnn1): GRU(7, 8)\n",
      "  (dropout): Dropout(p=0.25)\n",
      "  (dense1): Linear(in_features=8, out_features=4)\n",
      "  (dense2): Linear(in_features=4, out_features=1)\n",
      ")\n",
      "03:39:39 epoch 0 loss: 0.32524141669273376\n",
      "03:39:43 epoch 10 loss: 0.27353230118751526\n",
      "03:39:47 epoch 20 loss: 0.257537305355072\n",
      "03:39:51 epoch 30 loss: 0.2556832730770111\n",
      "03:39:55 epoch 40 loss: 0.2571830451488495\n",
      "03:39:59 epoch 50 loss: 0.25257718563079834\n",
      "03:40:03 epoch 60 loss: 0.25319257378578186\n",
      "03:40:07 epoch 70 loss: 0.2529506981372833\n",
      "03:40:11 epoch 80 loss: 0.251846045255661\n",
      "03:40:15 epoch 90 loss: 0.250905305147171\n",
      "03:40:19 epoch 100 loss: 0.25028932094573975\n",
      "03:40:24 epoch 110 loss: 0.24962270259857178\n",
      "03:40:27 epoch 120 loss: 0.2485026717185974\n",
      "03:40:32 epoch 130 loss: 0.2507774531841278\n",
      "03:40:36 epoch 140 loss: 0.24804501235485077\n",
      "03:40:40 epoch 150 loss: 0.2501901686191559\n",
      "03:40:43 epoch 160 loss: 0.2498587965965271\n",
      "03:40:47 epoch 170 loss: 0.24859170615673065\n",
      "03:40:51 epoch 180 loss: 0.24900923669338226\n",
      "03:40:56 epoch 190 loss: 0.24931728839874268\n",
      "03:41:00 epoch 200 loss: 0.2488418072462082\n",
      "03:41:04 epoch 210 loss: 0.24913837015628815\n",
      "03:41:08 epoch 220 loss: 0.24874632060527802\n",
      "03:41:12 epoch 230 loss: 0.2496405988931656\n",
      "03:41:16 epoch 240 loss: 0.24806548655033112\n",
      "03:41:20 epoch 250 loss: 0.24937593936920166\n",
      "03:41:24 epoch 260 loss: 0.24894247949123383\n",
      "03:41:28 epoch 270 loss: 0.24715428054332733\n",
      "03:41:32 epoch 280 loss: 0.24931687116622925\n",
      "03:41:36 epoch 290 loss: 0.2483600229024887\n",
      "03:41:40 epoch 300 loss: 0.24748176336288452\n",
      "03:41:44 epoch 310 loss: 0.2481033354997635\n",
      "03:41:48 epoch 320 loss: 0.24853551387786865\n",
      "03:41:52 epoch 330 loss: 0.2493436336517334\n",
      "03:41:56 epoch 340 loss: 0.2470947802066803\n",
      "03:42:00 epoch 350 loss: 0.24786485731601715\n",
      "03:42:05 epoch 360 loss: 0.24788813292980194\n",
      "03:42:09 epoch 370 loss: 0.24637152254581451\n",
      "03:42:13 epoch 380 loss: 0.2493165135383606\n",
      "03:42:17 epoch 390 loss: 0.2477199137210846\n",
      "03:42:21 epoch 400 loss: 0.2489452213048935\n",
      "03:42:25 epoch 410 loss: 0.24863867461681366\n",
      "03:42:29 epoch 420 loss: 0.2479071021080017\n",
      "03:42:33 epoch 430 loss: 0.24807706475257874\n",
      "03:42:37 epoch 440 loss: 0.2485400289297104\n",
      "03:42:41 epoch 450 loss: 0.24701346457004547\n",
      "03:42:45 epoch 460 loss: 0.24747933447360992\n",
      "03:42:49 epoch 470 loss: 0.24846015870571136\n",
      "03:42:53 epoch 480 loss: 0.24815373122692108\n",
      "03:42:58 epoch 490 loss: 0.24761530756950378\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CryptoNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_layers, hidden_size, drop_out_rate):\n",
    "        super(CryptoNet, self).__init__()\n",
    "        # set hidden size, layers and dropout rate\n",
    "        self.drop_out_rate = drop_out_rate\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # using a GRU (Gated Recurrent Unit), also try and LSTM\n",
    "        self.rnn1 = nn.GRU(input_size=input_size,\n",
    "                           hidden_size=self.hidden_size,\n",
    "                           num_layers=self.hidden_layers)\n",
    "        self.dropout = nn.Dropout(p=self.drop_out_rate)\n",
    "        self.dense1 = nn.Linear(self.hidden_size, 4)\n",
    "        self.dense2 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x_batch = x.view(len(x), 1, -1)\n",
    "        x_r, hidden = self.rnn1(x_batch, hidden)\n",
    "        x_d = self.dropout(x_r)\n",
    "        x_l = self.dense1(x_d)\n",
    "        x_l2 = self.dense2(x_l)\n",
    "        return x_l2, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.randn(self.hidden_layers, 1, self.hidden_size))\n",
    "\n",
    "\n",
    "# ## Train the RNN\n",
    "\n",
    "# Setup model for training and prediction\n",
    "torch.manual_seed(0)\n",
    "model = CryptoNet(hidden_layers=1, hidden_size=8, drop_out_rate=0.25)\n",
    "print(model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss(size_average=True)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.5)\n",
    "\n",
    "# Initialize the hidden layer during training, but keep it for later prediction.\n",
    "hidden = model.init_hidden()\n",
    "\n",
    "# Train the model on 500 epochs\n",
    "NUM_EPOCHS = 500\n",
    "for i in range(NUM_EPOCHS):\n",
    "    def closure():\n",
    "        model.zero_grad()\n",
    "        hidden = model.init_hidden()\n",
    "        out, hidden = model(x_train, hidden)\n",
    "        loss = criterion(out, y_train)\n",
    "        if i % 10 == 0:\n",
    "            print('{:%H:%M:%S} epoch {} loss: {}'.format(datetime.now(), i, loss.data.numpy()[0]), flush=True)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict over the holdout test set and retain the hidden state\n",
    "pred, new_hidden = model(x_pred, hidden)\n",
    "\n",
    "def unnormalize(x):\n",
    "  \"\"\"Undo the normalization step performed prior to training the model.\"\"\"\n",
    "  return (2. * x * df['close_price_change'].std())+df['close_price_change'].mean()\n",
    "\n",
    "# Unnormalize data and get close price\n",
    "predicted_value = unnormalize(pred.view(1).data.numpy()[0])\n",
    "previous_close_price = df.loc[last_day:].close_price.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the prediction and date value\n",
    "predicted_price = (predicted_value + 1)*previous_close_price\n",
    "prediction_date = pd.to_datetime(day_index_map.get(last_day), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN predicts the closing price for: \n",
      "2018-01-23 00:00:00 to be 10953.3473588 $\n"
     ]
    }
   ],
   "source": [
    "print(\"The RNN predicts the closing price for: \\n%s \\\n",
    "to be %s $\" % (prediction_date, predicted_price), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_out = [{'price_prediction': predicted_price,\n",
    "         'date': prediction_date.date(),\n",
    "         'date_updated': datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S\"),\n",
    "         'coin': 'bitcoin'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Persist Predictions\n",
    "\n",
    "# define the schema for this dataset\n",
    "schema = {\n",
    "    \"table_name\": \"crypto_predictions\",\n",
    "    \"options\": {\n",
    "        \"primary_key\": [\"coin\", \"date\", \"date_updated\"],\n",
    "        \"order_by\": [\"date asc\"]\n",
    "    },\n",
    "    \"columns\": {\n",
    "        \"coin\": \"text\",\n",
    "        \"date\": \"date\",\n",
    "        \"date_updated\": \"timestamp\",\n",
    "        \"price_prediction\": \"float\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to the data engine.\n",
      "2018-01-24 03:46:41,123 - skafossdk.data_engine - INFO - Saving Data ...\n",
      "2018-01-24 03:46:41,124 - skafossdk.data_engine - INFO - Sending msg...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Save out using the data engine\n",
    "print(\"Saving to the data engine.\", flush=True)\n",
    "skafos.engine.save(schema, data_out).result()\n",
    "print(\"Done.\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
